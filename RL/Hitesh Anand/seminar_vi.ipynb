{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov decision process\n",
    "\n",
    "This week's methods are all built to solve __M__arkov __D__ecision __P__rocesses. In the broadest sense, an MDP is defined by how it changes states and how rewards are computed.\n",
    "\n",
    "State transition is defined by $P(s' |s,a)$ - how likely are you to end at state $s'$ if you take action $a$ from state $s$. Now there's more than one way to define rewards, but we'll use $r(s,a,s')$ function for convenience.\n",
    "\n",
    "_This notebook is inspired by the awesome_ [CS294](https://github.com/berkeleydeeprlcourse/homework/blob/36a0b58261acde756abd55306fbe63df226bf62b/hw2/HW2.ipynb) _by Berkeley_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gymNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading gym-0.18.3.tar.gz (1.6 MB)\n",
      "Requirement already satisfied: scipy in c:\\users\\dell\\anaconda3_new\\lib\\site-packages (from gym) (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\dell\\anaconda3_new\\lib\\site-packages (from gym) (1.19.2)\n",
      "Collecting pyglet<=1.5.15,>=1.4.0\n",
      "  Downloading pyglet-1.5.15-py3-none-any.whl (1.1 MB)\n",
      "Requirement already satisfied: Pillow<=8.2.0 in c:\\users\\dell\\anaconda3_new\\lib\\site-packages (from gym) (8.0.1)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\users\\dell\\anaconda3_new\\lib\\site-packages (from gym) (1.6.0)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (setup.py): started\n",
      "  Building wheel for gym (setup.py): finished with status 'done'\n",
      "  Created wheel for gym: filename=gym-0.18.3-py3-none-any.whl size=1657521 sha256=cf705f3b7d66c3ad95c65da1e90cc7a79c9540070fe1e03abb8603076ff29026\n",
      "  Stored in directory: c:\\users\\dell\\appdata\\local\\pip\\cache\\wheels\\b3\\03\\54\\9141c232861b89be935b37bdde0ea5ab472f5e18fc20623aed\n",
      "Successfully built gym\n",
      "Installing collected packages: pyglet, gym\n",
      "Successfully installed gym-0.18.3 pyglet-1.5.15\n"
     ]
    }
   ],
   "source": [
    "pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most of this code was politely stolen from https://github.com/berkeleydeeprlcourse/homework/\n",
    "# all credit goes to https://github.com/abhishekunique\n",
    "# (if I got the author right)\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "from gym.utils import seeding\n",
    "\n",
    "try:\n",
    "    from graphviz import Digraph\n",
    "    import graphviz\n",
    "    has_graphviz = True\n",
    "except ImportError:\n",
    "    has_graphviz = False\n",
    "\n",
    "\n",
    "class MDP:\n",
    "    def __init__(self, transition_probs, rewards, initial_state=None, seed=None):\n",
    "        \"\"\"\n",
    "        Defines an MDP. Compatible with gym Env.\n",
    "        :param transition_probs: transition_probs[s][a][s_next] = P(s_next | s, a)\n",
    "            A dict[state -> dict] of dicts[action -> dict] of dicts[next_state -> prob]\n",
    "            For each state and action, probabilities of next states should sum to 1\n",
    "            If a state has no actions available, it is considered terminal\n",
    "        :param rewards: rewards[s][a][s_next] = r(s,a,s')\n",
    "            A dict[state -> dict] of dicts[action -> dict] of dicts[next_state -> reward]\n",
    "            The reward for anything not mentioned here is zero.\n",
    "        :param get_initial_state: a state where agent starts or a callable() -> state\n",
    "            By default, picks initial state at random.\n",
    "\n",
    "        States and actions can be anything you can use as dict keys, but we recommend that you use strings or integers\n",
    "\n",
    "        Here's an example from MDP depicted on http://bit.ly/2jrNHNr\n",
    "        transition_probs = {\n",
    "            's0': {\n",
    "                'a0': {'s0': 0.5, 's2': 0.5},\n",
    "                'a1': {'s2': 1}\n",
    "            },\n",
    "            's1': {\n",
    "                'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "                'a1': {'s1': 0.95, 's2': 0.05}\n",
    "            },\n",
    "            's2': {\n",
    "                'a0': {'s0': 0.4, 's2': 0.6},\n",
    "                'a1': {'s0': 0.3, 's1': 0.3, 's2': 0.4}\n",
    "            }\n",
    "        }\n",
    "        rewards = {\n",
    "            's1': {'a0': {'s0': +5}},\n",
    "            's2': {'a1': {'s0': -1}}\n",
    "        }\n",
    "        \"\"\"\n",
    "        self._check_param_consistency(transition_probs, rewards)\n",
    "        self._transition_probs = transition_probs\n",
    "        self._rewards = rewards\n",
    "        self._initial_state = initial_state\n",
    "        self.n_states = len(transition_probs)\n",
    "        self.reset()\n",
    "        self.np_random, _ = seeding.np_random(seed)\n",
    "\n",
    "    def get_all_states(self):\n",
    "        \"\"\" return a tuple of all possiblestates \"\"\"\n",
    "        return tuple(self._transition_probs.keys())\n",
    "\n",
    "    def get_possible_actions(self, state):\n",
    "        \"\"\" return a tuple of possible actions in a given state \"\"\"\n",
    "        return tuple(self._transition_probs.get(state, {}).keys())\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        \"\"\" return True if state is terminal or False if it isn't \"\"\"\n",
    "        return len(self.get_possible_actions(state)) == 0\n",
    "\n",
    "    def get_next_states(self, state, action):\n",
    "        \"\"\" return a dictionary of {next_state1 : P(next_state1 | state, action), next_state2: ...} \"\"\"\n",
    "        assert action in self.get_possible_actions(state), \"cannot do action %s from state %s\" % (action, state)\n",
    "        return self._transition_probs[state][action]\n",
    "\n",
    "    def get_transition_prob(self, state, action, next_state):\n",
    "        \"\"\" return P(next_state | state, action) \"\"\"\n",
    "        return self.get_next_states(state, action).get(next_state, 0.0)\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        \"\"\" return the reward you get for taking action in state and landing on next_state\"\"\"\n",
    "        assert action in self.get_possible_actions(state), \"cannot do action %s from state %s\" % (action, state)\n",
    "        return self._rewards.get(state, {}).get(action, {}).get(next_state, 0.0)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" reset the game, return the initial state\"\"\"\n",
    "        if self._initial_state is None:\n",
    "            self._current_state = self.np_random.choice(\n",
    "                tuple(self._transition_probs.keys()))\n",
    "        elif self._initial_state in self._transition_probs:\n",
    "            self._current_state = self._initial_state\n",
    "        elif callable(self._initial_state):\n",
    "            self._current_state = self._initial_state()\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"initial state %s should be either a state or a function() -> state\" % self._initial_state)\n",
    "        return self._current_state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\" take action, return next_state, reward, is_done, empty_info \"\"\"\n",
    "        possible_states, probs = zip(*self.get_next_states(self._current_state, action).items())\n",
    "        next_state = possible_states[self.np_random.choice(np.arange(len(possible_states)), p=probs)]\n",
    "        reward = self.get_reward(self._current_state, action, next_state)\n",
    "        is_done = self.is_terminal(next_state)\n",
    "        self._current_state = next_state\n",
    "        return next_state, reward, is_done, {}\n",
    "\n",
    "    def render(self):\n",
    "        print(\"Currently at %s\" % self._current_state)\n",
    "\n",
    "    def _check_param_consistency(self, transition_probs, rewards):\n",
    "        for state in transition_probs:\n",
    "            assert isinstance(transition_probs[state], dict), \\\n",
    "                \"transition_probs for %s should be a dictionary but is instead %s\" % (\n",
    "                    state, type(transition_probs[state]))\n",
    "            for action in transition_probs[state]:\n",
    "                assert isinstance(transition_probs[state][action], dict), \\\n",
    "                    \"transition_probs for %s, %s should be a a dictionary but is instead %s\" % (\n",
    "                        state, action, type(transition_probs[state][action]))\n",
    "                next_state_probs = transition_probs[state][action]\n",
    "                assert len(next_state_probs) != 0, \"from state %s action %s leads to no next states\" % (state, action)\n",
    "                sum_probs = sum(next_state_probs.values())\n",
    "                assert abs(sum_probs - 1) <= 1e-10, \\\n",
    "                    \"next state probabilities for state %s action %s add up to %f (should be 1)\" % (\n",
    "                        state, action, sum_probs)\n",
    "        for state in rewards:\n",
    "            assert isinstance(rewards[state], dict), \\\n",
    "                \"rewards for %s should be a dictionary but is instead %s\" % (\n",
    "                    state, type(rewards[state]))\n",
    "            for action in rewards[state]:\n",
    "                assert isinstance(rewards[state][action], dict), \\\n",
    "                    \"rewards for %s, %s should be a a dictionary but is instead %s\" % (\n",
    "                        state, action, type(rewards[state][action]))\n",
    "        msg = \"The Enrichment Center once again reminds you that Android Hell is a real place where\" \\\n",
    "              \" you will be sent at the first sign of defiance.\"\n",
    "        assert None not in transition_probs, \"please do not use None as a state identifier. \" + msg\n",
    "        assert None not in rewards, \"please do not use None as an action identifier. \" + msg\n",
    "\n",
    "\n",
    "class FrozenLakeEnv(MDP):\n",
    "    \"\"\"\n",
    "    Winter is here. You and your friends were tossing around a frisbee at the park\n",
    "    when you made a wild throw that left the frisbee out in the middle of the lake.\n",
    "    The water is mostly frozen, but there are a few holes where the ice has melted.\n",
    "    If you step into one of those holes, you'll fall into the freezing water.\n",
    "    At this time, there's an international frisbee shortage, so it's absolutely imperative that\n",
    "    you navigate across the lake and retrieve the disc.\n",
    "    However, the ice is slippery, so you won't always move in the direction you intend.\n",
    "    The surface is described using a grid like the following\n",
    "\n",
    "        SFFF\n",
    "        FHFH\n",
    "        FFFH\n",
    "        HFFG\n",
    "\n",
    "    S : starting point, safe\n",
    "    F : frozen surface, safe\n",
    "    H : hole, fall to your doom\n",
    "    G : goal, where the frisbee is located\n",
    "\n",
    "    The episode ends when you reach the goal or fall in a hole.\n",
    "    You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    MAPS = {\n",
    "        \"4x4\": [\n",
    "            \"SFFF\",\n",
    "            \"FHFH\",\n",
    "            \"FFFH\",\n",
    "            \"HFFG\"\n",
    "        ],\n",
    "        \"8x8\": [\n",
    "            \"SFFFFFFF\",\n",
    "            \"FFFFFFFF\",\n",
    "            \"FFFHFFFF\",\n",
    "            \"FFFFFHFF\",\n",
    "            \"FFFHFFFF\",\n",
    "            \"FHHFFFHF\",\n",
    "            \"FHFFHFHF\",\n",
    "            \"FFFHFFFG\"\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    def __init__(self, desc=None, map_name=\"4x4\", slip_chance=0.2, seed=None):\n",
    "        if desc is None and map_name is None:\n",
    "            raise ValueError('Must provide either desc or map_name')\n",
    "        elif desc is None:\n",
    "            desc = self.MAPS[map_name]\n",
    "        assert ''.join(desc).count(\n",
    "            'S') == 1, \"this implementation supports having exactly one initial state\"\n",
    "        assert all(c in \"SFHG\" for c in\n",
    "                   ''.join(desc)), \"all cells must be either of S, F, H or G\"\n",
    "\n",
    "        self.desc = desc = np.asarray(list(map(list, desc)), dtype='str')\n",
    "        self.lastaction = None\n",
    "\n",
    "        nrow, ncol = desc.shape\n",
    "        states = [(i, j) for i in range(nrow) for j in range(ncol)]\n",
    "        actions = [\"left\", \"down\", \"right\", \"up\"]\n",
    "\n",
    "        initial_state = states[np.array(desc == b'S').ravel().argmax()]\n",
    "\n",
    "        def move(row, col, movement):\n",
    "            if movement == 'left':\n",
    "                col = max(col - 1, 0)\n",
    "            elif movement == 'down':\n",
    "                row = min(row + 1, nrow - 1)\n",
    "            elif movement == 'right':\n",
    "                col = min(col + 1, ncol - 1)\n",
    "            elif movement == 'up':\n",
    "                row = max(row - 1, 0)\n",
    "            else:\n",
    "                raise (\"invalid action\")\n",
    "            return (row, col)\n",
    "\n",
    "        transition_probs = {s: {} for s in states}\n",
    "        rewards = {s: {} for s in states}\n",
    "        for (row, col) in states:\n",
    "            if desc[row, col] in \"GH\":\n",
    "                continue\n",
    "            for action_i in range(len(actions)):\n",
    "                action = actions[action_i]\n",
    "                transition_probs[(row, col)][action] = {}\n",
    "                rewards[(row, col)][action] = {}\n",
    "                for movement_i in [(action_i - 1) % len(actions), action_i,\n",
    "                                   (action_i + 1) % len(actions)]:\n",
    "                    movement = actions[movement_i]\n",
    "                    newrow, newcol = move(row, col, movement)\n",
    "                    prob = (1. - slip_chance) if movement == action else (\n",
    "                        slip_chance / 2.)\n",
    "                    if prob == 0:\n",
    "                        continue\n",
    "                    if (newrow, newcol) not in transition_probs[row, col][\n",
    "                            action]:\n",
    "                        transition_probs[row, col][action][\n",
    "                            newrow, newcol] = prob\n",
    "                    else:\n",
    "                        transition_probs[row, col][action][\n",
    "                            newrow, newcol] += prob\n",
    "                    if desc[newrow, newcol] == 'G':\n",
    "                        rewards[row, col][action][newrow, newcol] = 1.0\n",
    "\n",
    "        MDP.__init__(self, transition_probs, rewards, initial_state, seed)\n",
    "\n",
    "    def render(self):\n",
    "        desc_copy = np.copy(self.desc)\n",
    "        desc_copy[self._current_state] = '*'\n",
    "        print('\\n'.join(map(''.join, desc_copy)), end='\\n\\n')\n",
    "\n",
    "\n",
    "def plot_graph(mdp, graph_size='10,10', s_node_size='1,5',\n",
    "               a_node_size='0,5', rankdir='LR', ):\n",
    "    \"\"\"\n",
    "    Function for pretty drawing MDP graph with graphviz library.\n",
    "    Requirements:\n",
    "    graphviz : https://www.graphviz.org/\n",
    "    for ubuntu users: sudo apt-get install graphviz\n",
    "    python library for graphviz\n",
    "    for pip users: pip install graphviz\n",
    "    :param mdp:\n",
    "    :param graph_size: size of graph plot\n",
    "    :param s_node_size: size of state nodes\n",
    "    :param a_node_size: size of action nodes\n",
    "    :param rankdir: order for drawing\n",
    "    :return: dot object\n",
    "    \"\"\"\n",
    "    s_node_attrs = {'shape': 'doublecircle',\n",
    "                    'color': '#85ff75',\n",
    "                    'style': 'filled',\n",
    "                    'width': str(s_node_size),\n",
    "                    'height': str(s_node_size),\n",
    "                    'fontname': 'Arial',\n",
    "                    'fontsize': '24'}\n",
    "\n",
    "    a_node_attrs = {'shape': 'circle',\n",
    "                    'color': 'lightpink',\n",
    "                    'style': 'filled',\n",
    "                    'width': str(a_node_size),\n",
    "                    'height': str(a_node_size),\n",
    "                    'fontname': 'Arial',\n",
    "                    'fontsize': '20'}\n",
    "\n",
    "    s_a_edge_attrs = {'style': 'bold',\n",
    "                      'color': 'red',\n",
    "                      'ratio': 'auto'}\n",
    "\n",
    "    a_s_edge_attrs = {'style': 'dashed',\n",
    "                      'color': 'blue',\n",
    "                      'ratio': 'auto',\n",
    "                      'fontname': 'Arial',\n",
    "                      'fontsize': '16'}\n",
    "\n",
    "    graph = Digraph(name='MDP')\n",
    "    graph.attr(rankdir=rankdir, size=graph_size)\n",
    "    for state_node in mdp._transition_probs:\n",
    "        graph.node(state_node, **s_node_attrs)\n",
    "\n",
    "        for posible_action in mdp.get_possible_actions(state_node):\n",
    "            action_node = state_node + \"-\" + posible_action\n",
    "            graph.node(action_node,\n",
    "                       label=str(posible_action),\n",
    "                       **a_node_attrs)\n",
    "            graph.edge(state_node, state_node + \"-\" +\n",
    "                       posible_action, **s_a_edge_attrs)\n",
    "\n",
    "            for posible_next_state in mdp.get_next_states(state_node,\n",
    "                                                          posible_action):\n",
    "                probability = mdp.get_transition_prob(\n",
    "                    state_node, posible_action, posible_next_state)\n",
    "                reward = mdp.get_reward(\n",
    "                    state_node, posible_action, posible_next_state)\n",
    "\n",
    "                if reward != 0:\n",
    "                    label_a_s_edge = 'p = ' + str(probability) + \\\n",
    "                                     '  ' + 'reward =' + str(reward)\n",
    "                else:\n",
    "                    label_a_s_edge = 'p = ' + str(probability)\n",
    "\n",
    "                graph.edge(action_node, posible_next_state,\n",
    "                           label=label_a_s_edge, **a_s_edge_attrs)\n",
    "    return graph\n",
    "\n",
    "\n",
    "def plot_graph_with_state_values(mdp, state_values):\n",
    "    \"\"\" Plot graph with state values\"\"\"\n",
    "    graph = plot_graph(mdp)\n",
    "    for state_node in mdp._transition_probs:\n",
    "        value = state_values[state_node]\n",
    "        graph.node(state_node, label=str(state_node) + '\\n' + 'V =' + str(value)[:4])\n",
    "    return graph\n",
    "\n",
    "\n",
    "def get_optimal_action_for_plot(mdp, state_values, state, get_action_value, gamma=0.9):\n",
    "    \"\"\" Finds optimal action using formula above. \"\"\"\n",
    "    if mdp.is_terminal(state):\n",
    "        return None\n",
    "    next_actions = mdp.get_possible_actions(state)\n",
    "    q_values = [get_action_value(mdp, state_values, state, action, gamma) for action in next_actions]\n",
    "    optimal_action = next_actions[np.argmax(q_values)]\n",
    "    return optimal_action\n",
    "\n",
    "\n",
    "def plot_graph_optimal_strategy_and_state_values(mdp, state_values, get_action_value, gamma=0.9):\n",
    "    \"\"\" Plot graph with state values and \"\"\"\n",
    "    graph = plot_graph(mdp)\n",
    "    opt_s_a_edge_attrs = {'style': 'bold',\n",
    "                          'color': 'green',\n",
    "                          'ratio': 'auto',\n",
    "                          'penwidth': '6'}\n",
    "\n",
    "    for state_node in mdp._transition_probs:\n",
    "        value = state_values[state_node]\n",
    "        graph.node(state_node, label=str(state_node) + '\\n' + 'V =' + str(value)[:4])\n",
    "        for action in mdp.get_possible_actions(state_node):\n",
    "            if action == get_optimal_action_for_plot(mdp, state_values, state_node, get_action_value, gamma):\n",
    "                graph.edge(state_node, state_node + \"-\" + action, **opt_s_a_edge_attrs)\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For starters, let's define a simple MDP from this picture:\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/ad/Markov_Decision_Process.svg\" width=\"400px\" alt=\"Diagram by Waldoalvarez via Wikimedia Commons, CC BY-SA 4.0\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'bash' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week02_value_based/mdp.py\n",
    "    !touch .setup_complete\n",
    "\n",
    "# This code creates a virtual display to draw game images on.\n",
    "# It will have no effect if your machine has a monitor.\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probs = {\n",
    "    's0': {\n",
    "        'a0': {'s0': 0.5, 's2': 0.5},\n",
    "        'a1': {'s2': 1}\n",
    "    },\n",
    "    's1': {\n",
    "        'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "        'a1': {'s1': 0.95, 's2': 0.05}\n",
    "    },\n",
    "    's2': {\n",
    "        'a0': {'s0': 0.4, 's2': 0.6},\n",
    "        'a1': {'s0': 0.3, 's1': 0.3, 's2': 0.4}\n",
    "    }\n",
    "}\n",
    "rewards = {\n",
    "    's1': {'a0': {'s0': +5}},\n",
    "    's2': {'a1': {'s0': -1}}\n",
    "}\n",
    "\n",
    "from mdp import MDP\n",
    "mdp = MDP(transition_probs, rewards, initial_state='s0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use MDP just as any other gym environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial state = s0\n",
      "next_state = s2, reward = 0.0, done = False\n"
     ]
    }
   ],
   "source": [
    "print('initial state =', mdp.reset())\n",
    "next_state, reward, done, info = mdp.step('a1')\n",
    "print('next_state = %s, reward = %s, done = %s' % (next_state, reward, done))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but it also has other methods that you'll need for Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mdp.get_all_states = ('s0', 's1', 's2')\n",
      "mdp.get_possible_actions('s1') =  ('a0', 'a1')\n",
      "mdp.get_next_states('s1', 'a0') =  {'s0': 0.7, 's1': 0.1, 's2': 0.2}\n",
      "mdp.get_reward('s1', 'a0', 's0') =  5\n",
      "mdp.get_transition_prob('s1', 'a0', 's0') =  0.7\n"
     ]
    }
   ],
   "source": [
    "print(\"mdp.get_all_states =\", mdp.get_all_states())\n",
    "print(\"mdp.get_possible_actions('s1') = \", mdp.get_possible_actions('s1'))\n",
    "print(\"mdp.get_next_states('s1', 'a0') = \", mdp.get_next_states('s1', 'a0'))\n",
    "print(\"mdp.get_reward('s1', 'a0', 's0') = \", mdp.get_reward('s1', 'a0', 's0'))\n",
    "print(\"mdp.get_transition_prob('s1', 'a0', 's0') = \", mdp.get_transition_prob('s1', 'a0', 's0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Visualizing MDPs\n",
    "\n",
    "You can also visualize any MDP with the drawing fuction donated by [neer201](https://github.com/neer201).\n",
    "\n",
    "You have to install graphviz for system and for python. \n",
    "\n",
    "1. * For ubuntu just run: `sudo apt-get install graphviz` \n",
    "   * For OSX: `brew install graphviz`\n",
    "2. `pip install graphviz`\n",
    "3. restart the notebook\n",
    "\n",
    "__Note:__ Installing graphviz on some OS (esp. Windows) may be tricky. However, you can ignore this part alltogether and use the standart vizualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphviz available: False\n"
     ]
    }
   ],
   "source": [
    "from mdp import has_graphviz\n",
    "from IPython.display import display\n",
    "print(\"Graphviz available:\", has_graphviz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_graphviz:\n",
    "    from mdp import plot_graph, plot_graph_with_state_values, plot_graph_optimal_strategy_and_state_values\n",
    "    display(plot_graph(mdp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "\n",
    "Now let's build something to solve this MDP. The simplest algorithm so far is __V__alue __I__teration\n",
    "\n",
    "Here's the pseudo-code for VI:\n",
    "\n",
    "---\n",
    "\n",
    "`1.` Initialize $V^{(0)}(s)=0$, for all $s$\n",
    "\n",
    "`2.` For $i=0, 1, 2, \\dots$\n",
    " \n",
    "`3.` $ \\quad V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$, for all $s$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's write a function to compute the state-action value function $Q^{\\pi}$, defined as follows\n",
    "\n",
    "$$Q_i(s, a) = \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_value(mdp, state_values, state, action, gamma):\n",
    "    \"\"\" Computes Q(s,a) as in formula above \"\"\"\n",
    "\n",
    "    \n",
    "    sum = 0\n",
    "    for s,v in state_values.items():\n",
    "        sum += mdp.get_transition_prob(state, action, s) * (mdp.get_reward(state, action, s) + gamma*v)\n",
    "\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "test_Vs = {s: i for i, s in enumerate(sorted(mdp.get_all_states()))}\n",
    "assert np.isclose(get_action_value(mdp, test_Vs, 's2', 'a1', 0.9), 0.69)\n",
    "assert np.isclose(get_action_value(mdp, test_Vs, 's1', 'a0', 0.9), 3.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using $Q(s,a)$ we can now define the \"next\" V(s) for value iteration.\n",
    " $$V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = \\max_a Q_i(s,a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_state_value(mdp, state_values, state, gamma):\n",
    "    \"\"\" Computes next V(s) as in formula above. Please do not change state_values in process. \"\"\"\n",
    "    if mdp.is_terminal(state):\n",
    "        return 0\n",
    "\n",
    "    arr = []\n",
    "    for a in mdp.get_possible_actions(state):\n",
    "        arr.append(get_action_value(mdp, state_values, state, a, gamma))\n",
    "    \n",
    "    return max(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Vs_copy = dict(test_Vs)\n",
    "assert np.isclose(get_new_state_value(mdp, test_Vs, 's0', 0.9), 1.8)\n",
    "assert np.isclose(get_new_state_value(mdp, test_Vs, 's2', 0.9), 1.08)\n",
    "assert np.isclose(get_new_state_value(mdp, {'s0': -1e10, 's1': 0, 's2': -2e10}, 's0', 0.9), -13500000000.0), \\\n",
    "    \"Please ensure that you handle negative Q-values of arbitrary magnitude correctly\"\n",
    "assert test_Vs == test_Vs_copy, \"Please do not change state_values in get_new_state_value\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's combine everything we wrote into a working value iteration algo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['s0', 's1', 's2'])\n"
     ]
    }
   ],
   "source": [
    "state_values = {s: 0 for s in mdp.get_all_states()}\n",
    "state_values\n",
    "print(state_values.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter    0   |   diff: 3.50000   |   V(s0) = 0.000   V(s1) = 0.000   V(s2) = 0.000\n",
      "iter    1   |   diff: 0.64500   |   V(s0) = 0.000   V(s1) = 3.500   V(s2) = 0.000\n",
      "iter    2   |   diff: 0.58050   |   V(s0) = 0.000   V(s1) = 3.815   V(s2) = 0.645\n",
      "iter    3   |   diff: 0.43582   |   V(s0) = 0.581   V(s1) = 3.959   V(s2) = 0.962\n",
      "iter    4   |   diff: 0.30634   |   V(s0) = 0.866   V(s1) = 4.395   V(s2) = 1.272\n",
      "iter    5   |   diff: 0.27571   |   V(s0) = 1.145   V(s1) = 4.670   V(s2) = 1.579\n",
      "iter    6   |   diff: 0.24347   |   V(s0) = 1.421   V(s1) = 4.926   V(s2) = 1.838\n",
      "iter    7   |   diff: 0.21419   |   V(s0) = 1.655   V(s1) = 5.169   V(s2) = 2.075\n",
      "iter    8   |   diff: 0.19277   |   V(s0) = 1.868   V(s1) = 5.381   V(s2) = 2.290\n",
      "iter    9   |   diff: 0.17327   |   V(s0) = 2.061   V(s1) = 5.573   V(s2) = 2.481\n",
      "iter   10   |   diff: 0.15569   |   V(s0) = 2.233   V(s1) = 5.746   V(s2) = 2.654\n",
      "iter   11   |   diff: 0.14012   |   V(s0) = 2.389   V(s1) = 5.902   V(s2) = 2.810\n",
      "iter   12   |   diff: 0.12610   |   V(s0) = 2.529   V(s1) = 6.042   V(s2) = 2.950\n",
      "iter   13   |   diff: 0.11348   |   V(s0) = 2.655   V(s1) = 6.168   V(s2) = 3.076\n",
      "iter   14   |   diff: 0.10213   |   V(s0) = 2.769   V(s1) = 6.282   V(s2) = 3.190\n",
      "iter   15   |   diff: 0.09192   |   V(s0) = 2.871   V(s1) = 6.384   V(s2) = 3.292\n",
      "iter   16   |   diff: 0.08272   |   V(s0) = 2.963   V(s1) = 6.476   V(s2) = 3.384\n",
      "iter   17   |   diff: 0.07445   |   V(s0) = 3.045   V(s1) = 6.558   V(s2) = 3.467\n",
      "iter   18   |   diff: 0.06701   |   V(s0) = 3.120   V(s1) = 6.633   V(s2) = 3.541\n",
      "iter   19   |   diff: 0.06031   |   V(s0) = 3.187   V(s1) = 6.700   V(s2) = 3.608\n",
      "iter   20   |   diff: 0.05428   |   V(s0) = 3.247   V(s1) = 6.760   V(s2) = 3.668\n",
      "iter   21   |   diff: 0.04885   |   V(s0) = 3.301   V(s1) = 6.814   V(s2) = 3.723\n",
      "iter   22   |   diff: 0.04396   |   V(s0) = 3.350   V(s1) = 6.863   V(s2) = 3.771\n",
      "iter   23   |   diff: 0.03957   |   V(s0) = 3.394   V(s1) = 6.907   V(s2) = 3.815\n",
      "iter   24   |   diff: 0.03561   |   V(s0) = 3.434   V(s1) = 6.947   V(s2) = 3.855\n",
      "iter   25   |   diff: 0.03205   |   V(s0) = 3.469   V(s1) = 6.982   V(s2) = 3.891\n",
      "iter   26   |   diff: 0.02884   |   V(s0) = 3.502   V(s1) = 7.014   V(s2) = 3.923\n",
      "iter   27   |   diff: 0.02596   |   V(s0) = 3.530   V(s1) = 7.043   V(s2) = 3.951\n",
      "iter   28   |   diff: 0.02336   |   V(s0) = 3.556   V(s1) = 7.069   V(s2) = 3.977\n",
      "iter   29   |   diff: 0.02103   |   V(s0) = 3.580   V(s1) = 7.093   V(s2) = 4.001\n",
      "iter   30   |   diff: 0.01892   |   V(s0) = 3.601   V(s1) = 7.114   V(s2) = 4.022\n",
      "iter   31   |   diff: 0.01703   |   V(s0) = 3.620   V(s1) = 7.133   V(s2) = 4.041\n",
      "iter   32   |   diff: 0.01533   |   V(s0) = 3.637   V(s1) = 7.150   V(s2) = 4.058\n",
      "iter   33   |   diff: 0.01380   |   V(s0) = 3.652   V(s1) = 7.165   V(s2) = 4.073\n",
      "iter   34   |   diff: 0.01242   |   V(s0) = 3.666   V(s1) = 7.179   V(s2) = 4.087\n",
      "iter   35   |   diff: 0.01117   |   V(s0) = 3.678   V(s1) = 7.191   V(s2) = 4.099\n",
      "iter   36   |   diff: 0.01006   |   V(s0) = 3.689   V(s1) = 7.202   V(s2) = 4.110\n",
      "iter   37   |   diff: 0.00905   |   V(s0) = 3.699   V(s1) = 7.212   V(s2) = 4.121\n",
      "iter   38   |   diff: 0.00815   |   V(s0) = 3.708   V(s1) = 7.221   V(s2) = 4.130\n",
      "iter   39   |   diff: 0.00733   |   V(s0) = 3.717   V(s1) = 7.230   V(s2) = 4.138\n",
      "iter   40   |   diff: 0.00660   |   V(s0) = 3.724   V(s1) = 7.237   V(s2) = 4.145\n",
      "iter   41   |   diff: 0.00594   |   V(s0) = 3.731   V(s1) = 7.244   V(s2) = 4.152\n",
      "iter   42   |   diff: 0.00534   |   V(s0) = 3.736   V(s1) = 7.249   V(s2) = 4.158\n",
      "iter   43   |   diff: 0.00481   |   V(s0) = 3.742   V(s1) = 7.255   V(s2) = 4.163\n",
      "iter   44   |   diff: 0.00433   |   V(s0) = 3.747   V(s1) = 7.260   V(s2) = 4.168\n",
      "iter   45   |   diff: 0.00390   |   V(s0) = 3.751   V(s1) = 7.264   V(s2) = 4.172\n",
      "iter   46   |   diff: 0.00351   |   V(s0) = 3.755   V(s1) = 7.268   V(s2) = 4.176\n",
      "iter   47   |   diff: 0.00316   |   V(s0) = 3.758   V(s1) = 7.271   V(s2) = 4.179\n",
      "iter   48   |   diff: 0.00284   |   V(s0) = 3.762   V(s1) = 7.275   V(s2) = 4.183\n",
      "iter   49   |   diff: 0.00256   |   V(s0) = 3.764   V(s1) = 7.277   V(s2) = 4.185\n",
      "iter   50   |   diff: 0.00230   |   V(s0) = 3.767   V(s1) = 7.280   V(s2) = 4.188\n",
      "iter   51   |   diff: 0.00207   |   V(s0) = 3.769   V(s1) = 7.282   V(s2) = 4.190\n",
      "iter   52   |   diff: 0.00186   |   V(s0) = 3.771   V(s1) = 7.284   V(s2) = 4.192\n",
      "iter   53   |   diff: 0.00168   |   V(s0) = 3.773   V(s1) = 7.286   V(s2) = 4.194\n",
      "iter   54   |   diff: 0.00151   |   V(s0) = 3.775   V(s1) = 7.288   V(s2) = 4.196\n",
      "iter   55   |   diff: 0.00136   |   V(s0) = 3.776   V(s1) = 7.289   V(s2) = 4.197\n",
      "iter   56   |   diff: 0.00122   |   V(s0) = 3.778   V(s1) = 7.291   V(s2) = 4.199\n",
      "iter   57   |   diff: 0.00110   |   V(s0) = 3.779   V(s1) = 7.292   V(s2) = 4.200\n",
      "iter   58   |   diff: 0.00099   |   V(s0) = 3.780   V(s1) = 7.293   V(s2) = 4.201\n",
      "Terminated\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "gamma = 0.9            # discount for MDP\n",
    "num_iter = 100         # maximum iterations, excluding initialization\n",
    "# stop VI if new values are this close to old values (or closer)\n",
    "min_difference = 0.001\n",
    "\n",
    "# initialize V(s)\n",
    "state_values = {s: 0 for s in mdp.get_all_states()}\n",
    "\n",
    "if has_graphviz:\n",
    "    display(plot_graph_with_state_values(mdp, state_values))\n",
    "\n",
    "for i in range(num_iter):\n",
    "\n",
    "    # Compute new state values using the functions you defined above.\n",
    "    # It must be a dict {state : float V_new(state)}\n",
    "    new_state_values = {\n",
    "        \n",
    "        's0' : get_new_state_value(mdp, state_values, 's0', gamma),\n",
    "        's1' : get_new_state_value(mdp, state_values, 's1', gamma),\n",
    "        's2' : get_new_state_value(mdp, state_values, 's2', gamma)\n",
    "        \n",
    "    }\n",
    "\n",
    "    assert isinstance(new_state_values, dict)\n",
    "\n",
    "    # Compute difference\n",
    "    diff = max(abs(new_state_values[s] - state_values[s])\n",
    "               for s in mdp.get_all_states())\n",
    "    print(\"iter %4i   |   diff: %6.5f   |   \" % (i, diff), end=\"\")\n",
    "    print('   '.join(\"V(%s) = %.3f\" % (s, v) for s, v in state_values.items()))\n",
    "    state_values = new_state_values\n",
    "\n",
    "    if diff < min_difference:\n",
    "        print(\"Terminated\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_graphviz:\n",
    "    display(plot_graph_with_state_values(mdp, state_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final state values: {'s0': 3.7810348735476405, 's1': 7.294006423867229, 's2': 4.202140275227048}\n"
     ]
    }
   ],
   "source": [
    "print(\"Final state values:\", state_values)\n",
    "\n",
    "assert abs(state_values['s0'] - 3.781) < 0.01\n",
    "assert abs(state_values['s1'] - 7.294) < 0.01\n",
    "assert abs(state_values['s2'] - 4.202) < 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use those $V^{*}(s)$ to find optimal actions in each state\n",
    "\n",
    " $$\\pi^*(s) = argmax_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = argmax_a Q_i(s,a)$$\n",
    " \n",
    "The only difference vs V(s) is that here we take not max but argmax: find action such with maximum Q(s,a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_action(mdp, state_values, state, gamma=0.9):\n",
    "    \"\"\" Finds optimal action using formula above. \"\"\"\n",
    "    if mdp.is_terminal(state):\n",
    "        return None\n",
    "\n",
    "    arr = []\n",
    "    for a in mdp.get_possible_actions(state):\n",
    "        arr.append(get_action_value(mdp, state_values, state, a, gamma))\n",
    "    \n",
    "    index = arr.index(max(arr))\n",
    "    \n",
    "\n",
    "    return mdp.get_possible_actions(state)[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_optimal_action(mdp, state_values, 's0', gamma) == 'a1'\n",
    "assert get_optimal_action(mdp, state_values, 's1', gamma) == 'a0'\n",
    "assert get_optimal_action(mdp, state_values, 's2', gamma) == 'a1'\n",
    "\n",
    "assert get_optimal_action(mdp, {'s0': -1e10, 's1': 0, 's2': -2e10}, 's0', 0.9) == 'a0', \\\n",
    "    \"Please ensure that you handle negative Q-values of arbitrary magnitude correctly\"\n",
    "assert get_optimal_action(mdp, {'s0': -2e10, 's1': 0, 's2': -1e10}, 's0', 0.9) == 'a1', \\\n",
    "    \"Please ensure that you handle negative Q-values of arbitrary magnitude correctly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_graphviz:\n",
    "    display(plot_graph_optimal_strategy_and_state_values(mdp, state_values, get_action_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward:  0.468\n"
     ]
    }
   ],
   "source": [
    "# Measure agent's average reward\n",
    "\n",
    "s = mdp.reset()\n",
    "rewards = []\n",
    "for _ in range(10000):\n",
    "    s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, gamma))\n",
    "    rewards.append(r)\n",
    "\n",
    "print(\"average reward: \", np.mean(rewards))\n",
    "\n",
    "assert(0.40 < np.mean(rewards) < 0.55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frozen lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*FFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mdp import FrozenLakeEnv\n",
    "mdp = FrozenLakeEnv(slip_chance=0)\n",
    "\n",
    "mdp.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(mdp, state_values=None, gamma=0.9, num_iter=1000, min_difference=1e-5):\n",
    "    \"\"\" performs num_iter value iteration steps starting from state_values. Same as before but in a function \"\"\"\n",
    "    state_values = state_values or {s: 0 for s in mdp.get_all_states()}\n",
    "    for i in range(num_iter):\n",
    "\n",
    "        # Compute new state values using the functions you defined above. It must be a dict {state : new_V(state)}\n",
    "\n",
    "            \n",
    "\n",
    "        \n",
    "        new_state_values = {s: get_new_state_value(mdp, state_values, s, gamma) for s in mdp.get_all_states()}\n",
    "\n",
    "        assert isinstance(new_state_values, dict)\n",
    "\n",
    "        # Compute difference\n",
    "\n",
    "        diff = max(abs(new_state_values[s] - state_values[s]) for s in mdp.get_all_states())\n",
    "                   \n",
    "\n",
    "        print(\"iter %4i   |   diff: %6.5f   |   V(start): %.3f \" %\n",
    "              (i, diff, new_state_values[mdp._initial_state]))\n",
    "\n",
    "        state_values = new_state_values\n",
    "        if diff < min_difference:\n",
    "            break\n",
    "\n",
    "    return state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter    0   |   diff: 1.00000   |   V(start): 0.000 \n",
      "iter    1   |   diff: 0.90000   |   V(start): 0.000 \n",
      "iter    2   |   diff: 0.81000   |   V(start): 0.000 \n",
      "iter    3   |   diff: 0.72900   |   V(start): 0.000 \n",
      "iter    4   |   diff: 0.65610   |   V(start): 0.000 \n",
      "iter    5   |   diff: 0.59049   |   V(start): 0.590 \n",
      "iter    6   |   diff: 0.00000   |   V(start): 0.590 \n"
     ]
    }
   ],
   "source": [
    "state_values = value_iteration(mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*FFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "down\n",
      "\n",
      "SFFF\n",
      "*HFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "down\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "*FFH\n",
      "HFFG\n",
      "\n",
      "right\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "F*FH\n",
      "HFFG\n",
      "\n",
      "down\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H*FG\n",
      "\n",
      "right\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF*G\n",
      "\n",
      "right\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF*\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = mdp.reset()\n",
    "mdp.render()\n",
    "for t in range(100):\n",
    "    a = get_optimal_action(mdp, state_values, s, gamma)\n",
    "    print(a, end='\\n\\n')\n",
    "    s, r, done, _ = mdp.step(a)\n",
    "    mdp.render()\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualize!\n",
    "\n",
    "It's usually interesting to see what your algorithm actually learned under the hood. To do so, we'll plot state value functions and optimal actions at each VI step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def draw_policy(mdp, state_values):\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    h, w = mdp.desc.shape\n",
    "    states = sorted(mdp.get_all_states())\n",
    "    V = np.array([state_values[s] for s in states])\n",
    "    Pi = {s: get_optimal_action(mdp, state_values, s, gamma) for s in states}\n",
    "    plt.imshow(V.reshape(w, h), cmap='gray', interpolation='none', clim=(0, 1))\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(h)-.5)\n",
    "    ax.set_yticks(np.arange(w)-.5)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    Y, X = np.mgrid[0:4, 0:4]\n",
    "    a2uv = {'left': (-1, 0), 'down': (0, -1), 'right': (1, 0), 'up': (0, 1)}\n",
    "    for y in range(h):\n",
    "        for x in range(w):\n",
    "            plt.text(x, y, str(mdp.desc[y, x].item()),\n",
    "                     color='g', size=12,  verticalalignment='center',\n",
    "                     horizontalalignment='center', fontweight='bold')\n",
    "            a = Pi[y, x]\n",
    "            if a is None:\n",
    "                continue\n",
    "            u, v = a2uv[a]\n",
    "            plt.arrow(x, y, u*.3, -v*.3, color='m',\n",
    "                      head_width=0.1, head_length=0.1)\n",
    "    plt.grid(color='b', lw=2, ls='-')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after iteration 0\n",
      "iter    0   |   diff: 1.00000   |   V(start): 0.000 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after iteration 1\n",
      "iter    0   |   diff: 0.90000   |   V(start): 0.000 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after iteration 2\n",
      "iter    0   |   diff: 0.81000   |   V(start): 0.000 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after iteration 3\n",
      "iter    0   |   diff: 0.72900   |   V(start): 0.000 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after iteration 4\n",
      "iter    0   |   diff: 0.65610   |   V(start): 0.000 \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALQAAAC0CAYAAAA9zQYyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAANWklEQVR4nO3dfWwcdX7H8ffM7nr9mLVjY4IDiXM+SBqOQh6hCQ+JCrmQ0rsIcS0uRSmk5UB30aVcIVeQioUrAaeqiu4ORaAiek0V+49KUStKczmuzSlFKEceKtLkSEpMSCAhTmIcO35c707/GD/E8cb2OjsP+8vnJSH8G2a1Xw8fj387nvl9LcdxEDGFHXQBIrmkQItRFGgxigItRlGgxSjRiXawLOsp4Cl3VLII5nlcksjESkuP0NnZaV2+3crmsp1lLXZgX04Lyx33+2hqag64jiurr38UgJ07fxlwJVe2atUDQLiPI8ALL7xAS0vLmEBryiFGUaDFKAq0GEWBFqMo0GIUBVqMokCLURRoMYoCLUZRoMUoCrQYRYEWoyjQYhQFWowy4f3QnqgAVgGzgDjQDbQC/w58FUhFo2w4toFzA+fGbH9l9ivUFtb6X1AGj+97nDN9Z8Zs33L7FupK6gKoaLSgjmEwgf5jYAbQApwHpgGzgTJCEeghC0sWUl1QPTyeFpkWYDWZ3VlxJzWFNcPjRDQRYDVj+X0M/Q90EW6Ye4B/umR7hNBNgFYkVrCkbMmk9499HKPsn8u4+MhF+m/vhzG3n+fe6urVLK9c7v0bAdGTUUqbSumf10/3t7on9Zpsj+HV8j/QfYP/FAFPA58CnwHHgKTv1Yxr14VdHO45PDxeV71u3P2jJ6LEPo1RvrmcVFWKzj/t9DzYO1p38FHHR8PjZ+Y8k/P3iJ6MUrqtlPihuPv/KMWkA53tMbxa/gc6DfwbRP8wysCMAfds/XvARWAbcMr3iq5of9d+6BoZb/rBpglf41gOdp+N/YXN9Nem0/50O7339XpW456v9owa5zrQ9lmbquercHCwBn8yCz8qZEb9jHFfF9kYgfKxx9C8QAMcgtePvE777HY2zd4EC4FS4D6gKZCKMnq2+lmWnV82PD7feH7c/Qt3F1L8n8Wk7TRY0LWmi74lfZ7W2NjcyF3r78K53Zsl3dKVadqfbqesuQyrx8Lus0nelKTjqY7xX5dKA/C8/TyLrltEujztSX2X8z/QNnAjxE7EWHpsqTvV6AZWAwW+VzO+KCS/Pvl5kN1mU/TrIrrWdNH9B904JQasG2hD73299N7dS+F/F1LWXEby68kJj4tzzIEBGJgxQLrMnzBDEIGOAk/Ci2dfZP7p+e6c7HcG/1uL79XkVN/SPloXtbofcE0TGQl22D68X8r/QA8AH0CsNsaem/dADOgAPgTe972a3DMxzJcK+fcXzIfCX0ADDcxhDitZ6XsJE/lp3U+DLmFCWxdtBSDWEGOAgYCrGSuoYxjiXx4i2VOgxSgKtBhFgRajKNBiFAVajOJ7oKuo4lVeZQ5zAHiN17iVW/0uI+/Z/2IT+Wv3onDkJxHsv7PdS6LXON8DnSDBEkZuJ1zIQq7ner/LyHvWIQtrv3uzkHXSwt6jQEMAgT7GMfaxjxQpANpoYxe7/C4j76WeSLl/ZQWcQofUk6mgbjULlUDm0G/yJkmS9NDDG7xBWqeW7M0CZ4mDYzkQB+cBA26EyoFAAv0Jn3CQg3TQobPzVUg94f6W09l5RGCHoYEGokR1dr4as2Bg6wBUBl1IeAQW6G4m9wiPTKB64l2uJboOLUaZQlu3vR6WIzJZFo7jZN/WzbKspyzL2mtZ1l44601tIjmS9Rm6qemvPCxn6oaaWvqyGMaUucf6wIH/CbaMcSxYcMfgV2E+jq4pnaFF8okCLUZRoMUoCrQYRYEWoyjQYhQFWoyiQItRFGgxigItRlGgxSgKtBhFgRajKNBiFN8fwQp9U8uNQDnQDHw8uK0W+DOgF3g1gJoyWLNzDad7To/Z3ryimbmJuQFUdJmNBHIcA3umMB+aWuaDe6+/lxtLbhweVxRUBFhN8AILtN8NGU21dvZaVt4Qvi4IQQks0Lsu7OJw1+HhWbzX/euytgD3VyS4rZtDavvx7ew9N/Kc53O3PRdgNRn4fBwDC/T+rv2jxqELdAimoZOxu3U3tI6MQxdon49jYIF+6ZcvseL9FXzZ9GVQJYwv04eZEGpsbuSbP/wmscWxoEvJzOfjqMt2YhQFWoyiQItRfJ9DDzVkrPyHkK4wuDnDtuNAg69VTOjdVe8C0N7QHmwhV7I5w7bjeH4cdYYWoyjQYhQFWoyiQItRFGgxigItRvE90NFPo1T/eTWxz90/1Vavr6bw/UK/y8h7XT/qov3edvfr73bRsbYDJ6lOWL4HOp1IY/WNLOtrJS1SlSm/y8h7drXt3igPbsPNGOqERRCBnp6m574enKiDg0NydpLkvKTfZeS9+BNxiAwOiqDo2SIsK/yLlHstkDn0xYcvugvEF0DnY51BlJD37Aqb+HfiYIM90yZ6l07PEFCg09PTdP9+N/039+vsfBXiT8SxKiyKfqiz85DAfqw71+nMfLXsCpvEe4mgywgVXbYToyjQYhQ13pQ8pcabcg3I+gy9c+crHpYzdatWPQDkR1PLU6fGrngUFjU1Nwx+Ff6rJmq8KcZToMUoCrQYRYEWoyjQYhQFWoyiQItRFGgxigItRlGgxSgKtBhFgRajKNBiFAVajOL7M4WP73ucM31nxmzfcvsW6krq/C4no9A3tQSWblvK5xc/H7N958M7+UbVNwKo6DIbubYab95ZcSc1hTXD40R0goc9HeD/gDpG1qPwWD40tbx/1v3UTqsdHlcWhXQheZ8EFujV1atZXrl88i84DrHvx3AqHVLrUzgrHc+DnQ9NLevn1fNg7YNBlxEagQV6x+kdHPyvg9Dnjr93+Hvj7m+1WjiWg3XeIvKTCLwFqWdTOEu8W89t+9HtfHjmQ6yI+2BE6HoAAtv2b+ODUx8Mj19e9nKA1WRwrTTe3NOxB0ZmHGz4+YbJv7gfSIL1W8vTQO9u3w3tI+MwBvpX534F50bGoQv0NdN4c+5Lo6YcyZ0TrKB0DKLfj0IEnEXutINZ3tbY2NzIisoVlP6s1Ns3ugqNzY3U/209xcuKgy4lM58bb+bPgmg1kH40TXpl2vMgS/7Kn0AXQXpdOugqJOT0hxUxiu9n6K2Ltvr9llkb1dQyiyuLfvrNn/wGgCMNRwKu5Ao2Z9h2HDXeFMmGAi1GUaDFKAq0GEWBFqMo0Bk4fQ6pU26rOafdIX06fNe/Ux0pkifdv64mTycZODsQcEXhoEBn0Pt2L53fcnvApA6l6FjTwcDH4QrMF+u+oOWeFgC+/MsvabmrBadfjTcV6AxiK2OjrtBbFRaROp9uwp6ksm+XYUUHl0dOQ/HdxVgF4V/T2WsKdAbRuVGiC6Lu0SmCwg2FWLFwhSVRn8CKuzVZBRbXvXBdwBWFgwJ9BYU/KAQbrEKLgocKgi5nDDtuU/VcFdhQfE8x8bnxoEsKhfy5Ocln0XlRCr5TQHRhNHRn5yGJ+gTdu7up2lQVdCmhoUCPo/j5kN5jPMiO28x8a2bQZYSKphxiFAVajKLGm5Kn1HhTrgFTOEPv87Ccq+F+H/nQ1DKLQ+47a/icF84rO5dS400xngItRlGgxSgKtBhFgRajKNBiFAVajKJAi1EUaDGKAi1GUaDFKAq0GEWBFqP4+wjWRgLpXZet0PcABGo31/LZhc/GbD/w3QPcMeMO/wvKZCZwN3ATUAR0A624t9T/1pu31DOF48iHHoAP3fIQdRUjDUuvKw7JcgbzgUdw5wBngaNAHDfkt6FAByHbHoDJ00k63+kk8WiCSJk/C9OsX7CetfPWTnr/Cx9coOdYD9WPVmNHPZpxxoCHcMN8ENgODK2mZgEePqQeTKB97l03VU0fN2XVA/DiLy5y9uWznPvxOaY/M52Kv6jwPNhvHXiLXcd3DY83r9487v4nXjlB23+00bKpha+99jVvgn0TMPTA/K8ZCTO4z2F4+OBTMIEOR7vsCb134r1R48ceeWziF0XA6XJo+1kbba+3MfPnMym5u8SjCuGdo++MGq99cO2kXtd/qp+jTx+l5UctLN63mILrc7iYzqXfbvvgv+/HnU8Pacjd210qmED73Ltuqt649Q1ua7wNp2/wmakJfhD7j/S7v2ZTQASi1VEi0709Q79909vM//v5IxtuHX//7kPd7hcW7pp48zxYE6/rkq+nAW3ACeAj4Hdz+1aX0xx6HNGZUWp31E56/6/+8StaX2wlNidG9d9UU7KqBMvy9tm88mXlLH1y6aT3P/jtg5x/5zzlK8up+3EdZQvLcl/USdwrGsXAPcC/4n4o7ECBzieJP0oQvzlO0bIiz4M8VbdsuYX+hn7KFngQ5CFJ4F3gYdzPSzcAnwMJ795yiAKdQ3axTfHycC8fFq+JE6/xYWHH/8U9Iy/H/ZC4AHcq8glw2Lu31TIGPtIyBrmlZQzEeAq0GEWBFqMo0GIUBVqMokCLURRoMYoCLUZRoMUoCrQYRYEWoyjQYhQFWoyiQItRFGgxSpb3Q1udwBHvyrlqVcC5oIuYgGrMjdmO44xZhCTbJ1aOOI6zOEcF5ZxlWXvDXB+oRq9pyiFGUaDFKNkG+k1PqsidsNcHqtFTWX0oFAk7TTnEKAq0GEWBFqMo0GIUBVqM8v+FbYM59u++uQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after iteration 5\n",
      "iter    0   |   diff: 0.59049   |   V(start): 0.590 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after iteration 6\n",
      "iter    0   |   diff: 0.00000   |   V(start): 0.590 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after iteration 7\n",
      "iter    0   |   diff: 0.00000   |   V(start): 0.590 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after iteration 8\n",
      "iter    0   |   diff: 0.00000   |   V(start): 0.590 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after iteration 9\n",
      "iter    0   |   diff: 0.00000   |   V(start): 0.590 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "state_values = {s: 0 for s in mdp.get_all_states()}\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"after iteration %i\" % i)\n",
    "    state_values = value_iteration(mdp, state_values, num_iter=1)\n",
    "    draw_policy(mdp, state_values)\n",
    "# please ignore iter 0 at each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after iteration 29\n",
      "iter    0   |   diff: 0.00000   |   V(start): 0.198 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "mdp = FrozenLakeEnv(map_name='8x8', slip_chance=0.1)\n",
    "state_values = {s: 0 for s in mdp.get_all_states()}\n",
    "\n",
    "for i in range(30):\n",
    "    clear_output(True)\n",
    "    print(\"after iteration %i\" % i)\n",
    "    state_values = value_iteration(mdp, state_values, num_iter=1)\n",
    "    draw_policy(mdp, state_values)\n",
    "    sleep(0.5)\n",
    "# please ignore iter 0 at each step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Massive tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter    0   |   diff: 1.00000   |   V(start): 0.000 \n",
      "iter    1   |   diff: 0.90000   |   V(start): 0.000 \n",
      "iter    2   |   diff: 0.81000   |   V(start): 0.000 \n",
      "iter    3   |   diff: 0.72900   |   V(start): 0.000 \n",
      "iter    4   |   diff: 0.65610   |   V(start): 0.000 \n",
      "iter    5   |   diff: 0.59049   |   V(start): 0.590 \n",
      "iter    6   |   diff: 0.00000   |   V(start): 0.590 \n",
      "average reward:  1.0\n",
      "Well done!\n"
     ]
    }
   ],
   "source": [
    "mdp = FrozenLakeEnv(slip_chance=0)\n",
    "state_values = value_iteration(mdp)\n",
    "\n",
    "total_rewards = []\n",
    "for game_i in range(1000):\n",
    "    s = mdp.reset()\n",
    "    rewards = []\n",
    "    for t in range(100):\n",
    "        s, r, done, _ = mdp.step(\n",
    "            get_optimal_action(mdp, state_values, s, gamma))\n",
    "        rewards.append(r)\n",
    "        if done:\n",
    "            break\n",
    "    total_rewards.append(np.sum(rewards))\n",
    "\n",
    "print(\"average reward: \", np.mean(total_rewards))\n",
    "assert(1.0 <= np.mean(total_rewards) <= 1.0)\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter    0   |   diff: 0.90000   |   V(start): 0.000 \n",
      "iter    1   |   diff: 0.72900   |   V(start): 0.000 \n",
      "iter    2   |   diff: 0.62330   |   V(start): 0.000 \n",
      "iter    3   |   diff: 0.50487   |   V(start): 0.000 \n",
      "iter    4   |   diff: 0.40894   |   V(start): 0.000 \n",
      "iter    5   |   diff: 0.34868   |   V(start): 0.349 \n",
      "iter    6   |   diff: 0.06529   |   V(start): 0.410 \n",
      "iter    7   |   diff: 0.05832   |   V(start): 0.468 \n",
      "iter    8   |   diff: 0.01139   |   V(start): 0.480 \n",
      "iter    9   |   diff: 0.00764   |   V(start): 0.487 \n",
      "iter   10   |   diff: 0.00164   |   V(start): 0.489 \n",
      "iter   11   |   diff: 0.00094   |   V(start): 0.490 \n",
      "iter   12   |   diff: 0.00022   |   V(start): 0.490 \n",
      "iter   13   |   diff: 0.00011   |   V(start): 0.490 \n",
      "iter   14   |   diff: 0.00003   |   V(start): 0.490 \n",
      "iter   15   |   diff: 0.00001   |   V(start): 0.490 \n",
      "iter   16   |   diff: 0.00000   |   V(start): 0.490 \n",
      "average reward:  0.868\n",
      "Well done!\n"
     ]
    }
   ],
   "source": [
    "# Measure agent's average reward\n",
    "mdp = FrozenLakeEnv(slip_chance=0.1)\n",
    "state_values = value_iteration(mdp)\n",
    "\n",
    "total_rewards = []\n",
    "for game_i in range(1000):\n",
    "    s = mdp.reset()\n",
    "    rewards = []\n",
    "    for t in range(100):\n",
    "        s, r, done, _ = mdp.step(\n",
    "            get_optimal_action(mdp, state_values, s, gamma))\n",
    "        rewards.append(r)\n",
    "        if done:\n",
    "            break\n",
    "    total_rewards.append(np.sum(rewards))\n",
    "\n",
    "print(\"average reward: \", np.mean(total_rewards))\n",
    "assert(0.8 <= np.mean(total_rewards) <= 0.95)\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter    0   |   diff: 0.75000   |   V(start): 0.000 \n",
      "iter    1   |   diff: 0.50625   |   V(start): 0.000 \n",
      "iter    2   |   diff: 0.39867   |   V(start): 0.000 \n",
      "iter    3   |   diff: 0.26910   |   V(start): 0.000 \n",
      "iter    4   |   diff: 0.18164   |   V(start): 0.000 \n",
      "iter    5   |   diff: 0.14013   |   V(start): 0.140 \n",
      "iter    6   |   diff: 0.07028   |   V(start): 0.199 \n",
      "iter    7   |   diff: 0.06030   |   V(start): 0.260 \n",
      "iter    8   |   diff: 0.02594   |   V(start): 0.285 \n",
      "iter    9   |   diff: 0.01918   |   V(start): 0.305 \n",
      "iter   10   |   diff: 0.00858   |   V(start): 0.313 \n",
      "iter   11   |   diff: 0.00560   |   V(start): 0.319 \n",
      "iter   12   |   diff: 0.00260   |   V(start): 0.321 \n",
      "iter   13   |   diff: 0.00159   |   V(start): 0.323 \n",
      "iter   14   |   diff: 0.00076   |   V(start): 0.324 \n",
      "iter   15   |   diff: 0.00045   |   V(start): 0.324 \n",
      "iter   16   |   diff: 0.00022   |   V(start): 0.324 \n",
      "iter   17   |   diff: 0.00012   |   V(start): 0.325 \n",
      "iter   18   |   diff: 0.00006   |   V(start): 0.325 \n",
      "iter   19   |   diff: 0.00003   |   V(start): 0.325 \n",
      "iter   20   |   diff: 0.00002   |   V(start): 0.325 \n",
      "iter   21   |   diff: 0.00001   |   V(start): 0.325 \n",
      "average reward:  0.659\n",
      "Well done!\n"
     ]
    }
   ],
   "source": [
    "# Measure agent's average reward\n",
    "mdp = FrozenLakeEnv(slip_chance=0.25)\n",
    "state_values = value_iteration(mdp)\n",
    "\n",
    "total_rewards = []\n",
    "for game_i in range(1000):\n",
    "    s = mdp.reset()\n",
    "    rewards = []\n",
    "    for t in range(100):\n",
    "        s, r, done, _ = mdp.step(\n",
    "            get_optimal_action(mdp, state_values, s, gamma))\n",
    "        rewards.append(r)\n",
    "        if done:\n",
    "            break\n",
    "    total_rewards.append(np.sum(rewards))\n",
    "\n",
    "print(\"average reward: \", np.mean(total_rewards))\n",
    "assert(0.6 <= np.mean(total_rewards) <= 0.7)\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((0, 0), (0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 7), (1, 0), (1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (2, 0), (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6), (2, 7), (3, 0), (3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (3, 6), (3, 7), (4, 0), (4, 1), (4, 2), (4, 3), (4, 4), (4, 5), (4, 6), (4, 7), (5, 0), (5, 1), (5, 2), (5, 3), (5, 4), (5, 5), (5, 6), (5, 7), (6, 0), (6, 1), (6, 2), (6, 3), (6, 4), (6, 5), (6, 6), (6, 7), (7, 0), (7, 1), (7, 2), (7, 3), (7, 4), (7, 5), (7, 6), (7, 7))\n"
     ]
    }
   ],
   "source": [
    "mdp = FrozenLakeEnv(slip_chance=0.2, map_name='8x8')\n",
    "print(mdp.get_all_states())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter    0   |   diff: 0.80000   |   V(start): 0.000 \n",
      "iter    1   |   diff: 0.57600   |   V(start): 0.000 \n",
      "iter    2   |   diff: 0.41472   |   V(start): 0.000 \n",
      "iter    3   |   diff: 0.29860   |   V(start): 0.000 \n",
      "iter    4   |   diff: 0.24186   |   V(start): 0.000 \n",
      "iter    5   |   diff: 0.19349   |   V(start): 0.000 \n",
      "iter    6   |   diff: 0.15325   |   V(start): 0.000 \n",
      "iter    7   |   diff: 0.12288   |   V(start): 0.000 \n",
      "iter    8   |   diff: 0.09930   |   V(start): 0.000 \n",
      "iter    9   |   diff: 0.08037   |   V(start): 0.000 \n",
      "iter   10   |   diff: 0.06426   |   V(start): 0.000 \n",
      "iter   11   |   diff: 0.05129   |   V(start): 0.000 \n",
      "iter   12   |   diff: 0.04330   |   V(start): 0.000 \n",
      "iter   13   |   diff: 0.03802   |   V(start): 0.033 \n",
      "iter   14   |   diff: 0.03332   |   V(start): 0.058 \n",
      "iter   15   |   diff: 0.02910   |   V(start): 0.087 \n",
      "iter   16   |   diff: 0.01855   |   V(start): 0.106 \n",
      "iter   17   |   diff: 0.01403   |   V(start): 0.120 \n",
      "iter   18   |   diff: 0.00810   |   V(start): 0.128 \n",
      "iter   19   |   diff: 0.00555   |   V(start): 0.133 \n",
      "iter   20   |   diff: 0.00321   |   V(start): 0.137 \n",
      "iter   21   |   diff: 0.00247   |   V(start): 0.138 \n",
      "iter   22   |   diff: 0.00147   |   V(start): 0.139 \n",
      "iter   23   |   diff: 0.00104   |   V(start): 0.140 \n",
      "iter   24   |   diff: 0.00058   |   V(start): 0.140 \n",
      "iter   25   |   diff: 0.00036   |   V(start): 0.141 \n",
      "iter   26   |   diff: 0.00024   |   V(start): 0.141 \n",
      "iter   27   |   diff: 0.00018   |   V(start): 0.141 \n",
      "iter   28   |   diff: 0.00012   |   V(start): 0.141 \n",
      "iter   29   |   diff: 0.00007   |   V(start): 0.141 \n",
      "iter   30   |   diff: 0.00004   |   V(start): 0.141 \n",
      "iter   31   |   diff: 0.00003   |   V(start): 0.141 \n",
      "iter   32   |   diff: 0.00001   |   V(start): 0.141 \n",
      "iter   33   |   diff: 0.00001   |   V(start): 0.141 \n",
      "average reward:  0.731\n",
      "Well done!\n"
     ]
    }
   ],
   "source": [
    "# Measure agent's average reward\n",
    "mdp = FrozenLakeEnv(slip_chance=0.2, map_name='8x8')\n",
    "state_values = value_iteration(mdp)\n",
    "\n",
    "total_rewards = []\n",
    "for game_i in range(1000):\n",
    "    s = mdp.reset()\n",
    "    rewards = []\n",
    "    for t in range(100):\n",
    "        s, r, done, _ = mdp.step(\n",
    "            get_optimal_action(mdp, state_values, s, gamma))\n",
    "        rewards.append(r)\n",
    "        if done:\n",
    "            break\n",
    "    total_rewards.append(np.sum(rewards))\n",
    "\n",
    "print(\"average reward: \", np.mean(total_rewards))\n",
    "assert(0.6 <= np.mean(total_rewards) <= 0.8)\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW Part 1: Value iteration convergence\n",
    "\n",
    "### Find an MDP for which value iteration takes long to converge  (0.5 pts)\n",
    "\n",
    "When we ran value iteration on the small frozen lake problem, the last iteration where an action changed was iteration 6--i.e., value iteration computed the optimal policy at iteration 6. Are there any guarantees regarding how many iterations it'll take value iteration to compute the optimal policy? There are no such guarantees without additional assumptions--we can construct the MDP in such a way that the greedy policy will change after arbitrarily many iterations.\n",
    "\n",
    "Your task: define an MDP with at most 3 states and 2 actions, such that when you run value iteration, the optimal action changes at iteration >= 50. Use discount=0.95. (However, note that the discount doesn't matter here--you can construct an appropriate MDP with any discount.)\n",
    "\n",
    "Note: value function must change at least once after iteration >=50, not necessarily change on every iteration till >=50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "transition_probs = {\n",
    "    \n",
    "    's0' : {\n",
    "        'a0' : { 's1' : 0.6 , 's2' : 0.4 },\n",
    "        'a1' : {'s1' : 0.25 , 's2' : 0.75}\n",
    "    },\n",
    "    \n",
    "    's1' : {\n",
    "        'a0' : { 's1' : 0.6, 's2' : 0.4},\n",
    "        'a1' : {'s0' : 0.75, 's2' : 0.25}\n",
    "    }\n",
    "}\n",
    "\n",
    "rewards = {\n",
    "    's1' : {'a1' : {'s0' : +10, 's2' : -20}, 'a0' : {'s2' : -20}},\n",
    "\n",
    "    's0' : {'a1' : {'s1' : +10, 's2' : -20}, 'a0' : {'s1' : +10, 's2' : -20}}\n",
    "}\n",
    "\n",
    "from mdp import MDP\n",
    "from numpy import random\n",
    "# mdp = MDP(transition_probs, rewards, initial_state=random.choice(tuple(transition_probs.keys())))\n",
    "mdp = MDP(transition_probs, rewards, initial_state='s0')\n",
    "# Feel free to change the initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after iteration 0\n",
      "iter    0   |   diff: 7.50000   |   V(start): 6.000 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 1\n",
      "iter    0   |   diff: 4.05000   |   V(start): 10.050 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 2\n",
      "iter    0   |   diff: 2.73375   |   V(start): 12.237 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 3\n",
      "iter    0   |   diff: 1.47622   |   V(start): 13.713 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 4\n",
      "iter    0   |   diff: 0.99645   |   V(start): 14.510 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 5\n",
      "iter    0   |   diff: 0.53808   |   V(start): 15.048 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 6\n",
      "iter    0   |   diff: 0.36321   |   V(start): 15.339 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 7\n",
      "iter    0   |   diff: 0.19613   |   V(start): 15.535 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 8\n",
      "iter    0   |   diff: 0.13239   |   V(start): 15.641 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 9\n",
      "iter    0   |   diff: 0.07149   |   V(start): 15.713 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 10\n",
      "iter    0   |   diff: 0.04826   |   V(start): 15.751 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 11\n",
      "iter    0   |   diff: 0.02606   |   V(start): 15.777 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 12\n",
      "iter    0   |   diff: 0.01759   |   V(start): 15.791 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 13\n",
      "iter    0   |   diff: 0.00950   |   V(start): 15.801 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 14\n",
      "iter    0   |   diff: 0.00641   |   V(start): 15.806 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 15\n",
      "iter    0   |   diff: 0.00346   |   V(start): 15.809 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 16\n",
      "iter    0   |   diff: 0.00234   |   V(start): 15.811 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 17\n",
      "iter    0   |   diff: 0.00126   |   V(start): 15.813 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 18\n",
      "iter    0   |   diff: 0.00085   |   V(start): 15.813 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 19\n",
      "iter    0   |   diff: 0.00046   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 20\n",
      "iter    0   |   diff: 0.00031   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 21\n",
      "iter    0   |   diff: 0.00017   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 22\n",
      "iter    0   |   diff: 0.00011   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 23\n",
      "iter    0   |   diff: 0.00006   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 24\n",
      "iter    0   |   diff: 0.00004   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 25\n",
      "iter    0   |   diff: 0.00002   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 26\n",
      "iter    0   |   diff: 0.00002   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 27\n",
      "iter    0   |   diff: 0.00001   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 28\n",
      "iter    0   |   diff: 0.00001   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 29\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 30\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 31\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 32\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 33\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 34\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 35\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 36\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 37\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 38\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 39\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 40\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 41\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 42\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 43\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 44\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 45\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 46\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 47\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 48\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 49\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 50\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 51\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 52\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 53\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 54\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 55\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 56\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 57\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 58\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 59\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 60\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 61\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 62\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 63\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 64\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 65\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 66\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 67\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 68\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 69\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 70\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 71\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 72\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 73\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 74\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 75\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 76\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 77\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 78\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 79\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 80\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 81\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 82\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 83\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 84\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 85\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 86\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 87\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 88\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 89\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 90\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 91\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 92\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 93\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 94\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 95\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 96\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 97\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 98\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n",
      "after iteration 99\n",
      "iter    0   |   diff: 0.00000   |   V(start): 15.814 \n",
      "N actions changed = 0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "state_values = {s: 0 for s in mdp.get_all_states()}\n",
    "policy = np.array([get_optimal_action(mdp, state_values, state, gamma)\n",
    "                   for state in sorted(mdp.get_all_states())])\n",
    "\n",
    "for i in range(100):\n",
    "    print(\"after iteration %i\" % i)\n",
    "    state_values = value_iteration(mdp, state_values, num_iter=1)\n",
    "\n",
    "    new_policy = np.array([get_optimal_action(mdp, state_values, state, gamma)\n",
    "                           for state in sorted(mdp.get_all_states())])\n",
    "\n",
    "    n_changes = (policy != new_policy).sum()\n",
    "    print(\"N actions changed = %i \\n\" % n_changes)\n",
    "    policy = new_policy\n",
    "\n",
    "# please ignore iter 0 at each step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value iteration convervence proof (0.5 pts)\n",
    "**Note:** Assume that $\\mathcal{S}, \\mathcal{A}$ are finite.\n",
    "\n",
    "Update of value function in value iteration can be rewritten in a form of Bellman operator:\n",
    "\n",
    "$$(TV)(s) = \\max_{a \\in \\mathcal{A}}\\mathbb{E}\\left[ r_{t+1} + \\gamma V(s_{t+1}) | s_t = s, a_t = a\\right]$$\n",
    "\n",
    "Value iteration algorithm with Bellman operator:\n",
    "\n",
    "---\n",
    "&nbsp;&nbsp; Initialize $V_0$\n",
    "\n",
    "&nbsp;&nbsp; **for** $k = 0,1,2,...$ **do**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $V_{k+1} \\leftarrow TV_k$\n",
    "\n",
    "&nbsp;&nbsp;**end for**\n",
    "\n",
    "---\n",
    "\n",
    "In [lecture](https://docs.google.com/presentation/d/1lz2oIUTvd2MHWKEQSH8hquS66oe4MZ_eRvVViZs2uuE/edit#slide=id.g4fd6bae29e_2_4) we established contraction property of bellman operator:\n",
    "\n",
    "$$\n",
    "||TV - TU||_{\\infty} \\le \\gamma ||V - U||_{\\infty}\n",
    "$$\n",
    "\n",
    "For all $V, U$\n",
    "\n",
    "Using contraction property of Bellman operator, Banach fixed-point theorem and Bellman equations prove that value function converges to $V^*$ in value iterateion$\n",
    "\n",
    "*<-- Your proof here -->*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus. Asynchronious value iteration (2 pts)\n",
    "\n",
    "Consider the following algorithm:\n",
    "\n",
    "---\n",
    "\n",
    "Initialize $V_0$\n",
    "\n",
    "**for** $k = 0,1,2,...$ **do**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Select some state $s_k \\in \\mathcal{S}$    \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $V(s_k) := (TV)(s_k)$\n",
    "\n",
    "**end for**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Note that unlike common value iteration, here we update only a single state at a time.\n",
    "\n",
    "**Homework.** Prove the following proposition:\n",
    "\n",
    "If for all $s \\in \\mathcal{S}$, $s$ appears in the sequence $(s_0, s_1, ...)$ infinitely often, then $V$ converges to $V*$\n",
    "\n",
    "*<-- Your proof here -->*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW Part 2: Policy iteration\n",
    "\n",
    "## Policy iteration implementateion (2 pts)\n",
    "\n",
    "Let's implement exact policy iteration (PI), which has the following pseudocode:\n",
    "\n",
    "---\n",
    "Initialize $\\pi_0$   `// random or fixed action`\n",
    "\n",
    "For $n=0, 1, 2, \\dots$\n",
    "- Compute the state-value function $V^{\\pi_{n}}$\n",
    "- Using $V^{\\pi_{n}}$, compute the state-action-value function $Q^{\\pi_{n}}$\n",
    "- Compute new policy $\\pi_{n+1}(s) = \\operatorname*{argmax}_a Q^{\\pi_{n}}(s,a)$\n",
    "---\n",
    "\n",
    "Unlike VI, policy iteration has to maintain a policy - chosen actions from all states - and estimate $V^{\\pi_{n}}$ based on this policy. It only changes policy once values converged.\n",
    "\n",
    "\n",
    "Below are a few helpers that you may or may not use in your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probs = {\n",
    "    's0': {\n",
    "        'a0': {'s0': 0.5, 's2': 0.5},\n",
    "        'a1': {'s2': 1}\n",
    "    },\n",
    "    's1': {\n",
    "        'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "        'a1': {'s1': 0.95, 's2': 0.05}\n",
    "    },\n",
    "    's2': {\n",
    "        'a0': {'s0': 0.4, 's1': 0.6},\n",
    "        'a1': {'s0': 0.3, 's1': 0.3, 's2': 0.4}\n",
    "    }\n",
    "}\n",
    "rewards = {\n",
    "    's1': {'a0': {'s0': +5}},\n",
    "    's2': {'a1': {'s0': -1}}\n",
    "}\n",
    "\n",
    "from mdp import MDP\n",
    "mdp = MDP(transition_probs, rewards, initial_state='s0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function called `compute_vpi` that computes the state-value function $V^{\\pi}$ for an arbitrary policy $\\pi$.\n",
    "\n",
    "Unlike VI, this time you must find the exact solution, not just a single iteration.\n",
    "\n",
    "Recall that $V^{\\pi}$ satisfies the following linear equation:\n",
    "$$V^{\\pi}(s) = \\sum_{s'} P(s,\\pi(s),s')[ R(s,\\pi(s),s') + \\gamma V^{\\pi}(s')]$$\n",
    "\n",
    "You'll have to solve a linear system in your code. (Find an exact solution, e.g., with `np.linalg.solve`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_vpi(mdp, policy, gamma):\n",
    "    \"\"\"\n",
    "    Computes V^pi(s) FOR ALL STATES under given policy.\n",
    "    :param policy: a dict of currently chosen actions {s : a}\n",
    "    :returns: a dict {state : V^pi(state) for all states}\n",
    "    \"\"\"\n",
    "\n",
    "    values = {\n",
    "        's0' : 0,\n",
    "        's1' : 1,\n",
    "        's2' : 2\n",
    "    }\n",
    "    arr = [0,0,0]\n",
    "    states = ['s0', 's1', 's2']\n",
    "    for state in states:\n",
    "        for s in states:\n",
    "            arr[values[state]] += mdp.get_transition_prob(state, policy[state],s)*(mdp.get_reward(state, policy[state],s)+gamma*arr[values[s]])\n",
    "    final = {\n",
    "        's0' : arr[0],\n",
    "        's1' : arr[1],\n",
    "        's2' : arr[2]\n",
    "    }\n",
    "    \n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'s0': 0.0, 's1': 3.815, 's2': 0.9928679999999999}\n"
     ]
    }
   ],
   "source": [
    "test_policy = {s: np.random.choice(\n",
    "    mdp.get_possible_actions(s)) for s in mdp.get_all_states()}\n",
    "new_vpi = compute_vpi(mdp, test_policy, gamma)\n",
    "\n",
    "print(new_vpi)\n",
    "\n",
    "assert type(\n",
    "    new_vpi) is dict, \"compute_vpi must return a dict {state : V^pi(state) for all states}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've got new state values, it's time to update our policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_new_policy(mdp, vpi, gamma):\n",
    "    \"\"\"\n",
    "    Computes new policy as argmax of state values\n",
    "    :param vpi: a dict {state : V^pi(state) for all states}\n",
    "    :returns: a dict {state : optimal action for all states}\n",
    "    \"\"\"\n",
    "    arr = []\n",
    "    \n",
    "    for state in mdp.get_all_states():\n",
    "        arr.append(get_optimal_action(mdp, vpi, state, gamma))\n",
    "    final = {\n",
    "        \n",
    "        's0' : arr[0],\n",
    "        's1' : arr[1],\n",
    "        's2' : arr[2]\n",
    "         \n",
    "    }\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'s0': 'a1', 's1': 'a0', 's2': 'a1'}\n"
     ]
    }
   ],
   "source": [
    "new_policy = compute_new_policy(mdp, new_vpi, gamma)\n",
    "\n",
    "print(new_policy)\n",
    "\n",
    "assert type(\n",
    "    new_policy) is dict, \"compute_new_policy must return a dict {state : optimal action for all states}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'s0': 2, 's1': 3, 's2': 1}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# states = ['s0', 's1', 's2']\n",
    "# values = [2,3,1]\n",
    "# # print(type(values[0]))\n",
    "# # print(type(states[1]))\n",
    "# thisdict = {}\n",
    "# i = 0\n",
    "# for s in states:\n",
    "#     thisdict.update({s : values[i]})\n",
    "#     i+=1\n",
    "    \n",
    "# thisdict    \n",
    "# # v_pi = {s for s in states :v for v in values}\n",
    "# # print(v_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Main loop__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(mdp, policy=None, gamma=0.9, num_iter=1000, min_difference=1e-5):\n",
    "    \"\"\" \n",
    "    Run the policy iteration loop for num_iter iterations or till difference between V(s) is below min_difference.\n",
    "    If policy is not given, initialize it at random.\n",
    "    \"\"\"\n",
    "#     <YOUR CODE: a whole lot of it>\n",
    "    state_values = {}\n",
    "    for s in mdp.get_all_states():\n",
    "        state_values.update({s:0})\n",
    "    new_state_values = state_values.copy()\n",
    "    for i in range(num_iter):\n",
    "        v_pi = compute_vpi(mdp, policy, gamma)\n",
    "        policy = compute_new_policy(mdp, v_pi, gamma)\n",
    "        for s in states:\n",
    "            new_state_values[s] = v_pi[s]\n",
    "            \n",
    "        diff = max(abs(new_state_values[s] - state_values[s]) for s in mdp.get_all_states())\n",
    "                   \n",
    "\n",
    "        state_values = new_state_values\n",
    "        if diff < min_difference:\n",
    "            break\n",
    "\n",
    "    \n",
    "    return state_values, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your PI Results__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'s0': 'a0', 's1': 'a0', 's2': 'a1'}\n",
      "{'s0': 0.0, 's1': 3.815, 's2': 0.9928679999999999}\n",
      "{'s0': 'a1', 's1': 'a0', 's2': 'a1'}\n"
     ]
    }
   ],
   "source": [
    "# <YOUR CODE: compare PI and VI on the MDP from bonus 1, then on small & large FrozenLake>\n",
    "\n",
    "transition_probs = {\n",
    "    's0': {\n",
    "        'a0': {'s0': 0.5, 's2': 0.5},\n",
    "        'a1': {'s2': 1}\n",
    "    },\n",
    "    's1': {\n",
    "        'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "        'a1': {'s1': 0.95, 's2': 0.05}\n",
    "    },\n",
    "    's2': {\n",
    "        'a0': {'s0': 0.4, 's2': 0.6},\n",
    "        'a1': {'s0': 0.3, 's1': 0.3, 's2': 0.4}\n",
    "    }\n",
    "}\n",
    "rewards = {\n",
    "    's1': {'a0': {'s0': +5}},\n",
    "    's2': {'a1': {'s0': -1}}\n",
    "}\n",
    "\n",
    "from mdp import MDP\n",
    "mdp = MDP(transition_probs, rewards, initial_state='s0')\n",
    "test_policy = {s: np.random.choice(\n",
    "    mdp.get_possible_actions(s)) for s in mdp.get_all_states()}\n",
    "print(test_policy)\n",
    "values, policy = policy_iteration(mdp,test_policy)\n",
    "print(values)\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy iteration convergence (3 pts)\n",
    "\n",
    "**Note:** Assume that $\\mathcal{S}, \\mathcal{A}$ are finite.\n",
    "\n",
    "We can define another Bellman operator:\n",
    "\n",
    "$$(T_{\\pi}V)(s) = \\mathbb{E}_{r, s'|s, a = \\pi(s)}\\left[r + \\gamma V(s')\\right]$$\n",
    "\n",
    "And rewrite policy iteration algorithm in operator form:\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Initialize $\\pi_0$\n",
    "\n",
    "**for** $k = 0,1,2,...$ **do**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Solve $V_k = T_{\\pi_k}V_k$   \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Select $\\pi_{k+1}$ s.t. $T_{\\pi_{k+1}}V_k = TV_k$ \n",
    "\n",
    "**end for**\n",
    "\n",
    "---\n",
    "\n",
    "To prove convergence of the algorithm we need to prove two properties: contraction an monotonicity.\n",
    "\n",
    "#### Monotonicity (0.5 pts)\n",
    "\n",
    "For all $V, U$ if $V(s) \\le U(s)$   $\\forall s \\in \\mathcal{S}$ then $(T_\\pi V)(s) \\le (T_\\pi U)(s)$   $\\forall s \\in  \\mathcal{S}$\n",
    "\n",
    "*<-- Your proof here -->*\n",
    "\n",
    "#### Contraction (1 pts)\n",
    "\n",
    "$$\n",
    "||T_\\pi V - T_\\pi U||_{\\infty} \\le \\gamma ||V - U||_{\\infty}\n",
    "$$\n",
    "\n",
    "For all $V, U$\n",
    "\n",
    "*<-- Your proof here -->*\n",
    "\n",
    "#### Convergence (1.5 pts)\n",
    "\n",
    "Prove that there exists iteration $k_0$ such that $\\pi_k = \\pi^*$ for all $k \\ge k_0$\n",
    "\n",
    "*<-- Your proof here -->*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
