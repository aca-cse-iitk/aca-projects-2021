{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WEEK 2 Submission by TEAM3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOZSbwxERcyR9Q2gvXuzmnV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shreyasi2002/ACA-Wikipedia-Simplifier/blob/main/WEEK_2_Submission_by_TEAM3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bs6KQXq9vPi-"
      },
      "source": [
        "# ***WEEK 2 ASSIGNMENT SUBMISSION***\n",
        "\n",
        "***TEAM 3***\n",
        "\n",
        "\n",
        "> *- Shreyasi Mandal*\n",
        "\n",
        "> *- Anushka Panda*\n",
        "\n",
        "> *- Lakshmi Pravallika*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T08WYF97tZse"
      },
      "source": [
        "# ***AND GATE USING NEURAL NETWORK***\n",
        "\n",
        "*We are going to develop an AND gate using an artificial neural network*\n",
        "\n",
        "***Some Terms***\n",
        "\n",
        "\n",
        "*  *Forward Propagation is used to compute the activations from the input, then hidden then the output layer* \n",
        "*  *What a Neural Network does is the same as logistic regression*\n",
        "*   *In Back-Propagation we start by computing the delta term for the output layer and then back-propagate to compute the delta terms of the hidden layers*\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKvDYWzggpE1",
        "outputId": "f5a676fa-70fc-49e0-80a3-0708534cf0ec"
      },
      "source": [
        "# import Python Libraries\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Sigmoid Function to find out hypothesis\n",
        "def sigmoid(z):\n",
        "\treturn 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Initialization of the neural network parameters\n",
        "# Initialized all the weights in the range of between 0 and 1\n",
        "# Bias values are initialized to 0\n",
        "def initializeParameters(inputFeatures, neuronsInHiddenLayers, outputFeatures):\n",
        "\tW1 = np.random.randn(neuronsInHiddenLayers, inputFeatures)\n",
        "\tW2 = np.random.randn(outputFeatures, neuronsInHiddenLayers)\n",
        "\tb1 = np.zeros((neuronsInHiddenLayers, 1))\n",
        "\tb2 = np.zeros((outputFeatures, 1))\n",
        "\t\n",
        "\tparameters = {\"W1\" : W1, \"b1\": b1,\n",
        "\t\t\t\t\"W2\" : W2, \"b2\": b2}\n",
        "\t\n",
        "  return parameters\n",
        "\n",
        "# Forward Propagation in Neural Networks\n",
        "def forwardPropagation(X, Y, parameters):\n",
        "\tm = X.shape[1]\n",
        "\tW1 = parameters[\"W1\"]\n",
        "\tW2 = parameters[\"W2\"]\n",
        "\tb1 = parameters[\"b1\"]\n",
        "\tb2 = parameters[\"b2\"]\n",
        "\n",
        "\tZ1 = np.dot(W1, X) + b1  # Performing multiplication in matrices\n",
        "\tA1 = sigmoid(Z1)   # Calling the sigmoid function\n",
        "\tZ2 = np.dot(W2, A1) + b2\n",
        "\tA2 = sigmoid(Z2)\n",
        "\n",
        "\tcache = (Z1, A1, W1, b1, Z2, A2, W2, b2)\n",
        "\tlogprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), (1 - Y))\n",
        "\tcost = -np.sum(logprobs) / m\n",
        "\treturn cost, cache, A2\n",
        "\n",
        "# Backward Propagation to compute partial derivatives (aka gradients)\n",
        "def backwardPropagation(X, Y, cache):\n",
        "\tm = X.shape[1]\n",
        "\t(Z1, A1, W1, b1, Z2, A2, W2, b2) = cache\n",
        "\t\n",
        "\tdZ2 = A2 - Y\n",
        "\tdW2 = np.dot(dZ2, A1.T) / m\n",
        "\tdb2 = np.sum(dZ2, axis = 1, keepdims = True)\n",
        "\t\n",
        "\tdA1 = np.dot(W2.T, dZ2)\n",
        "\tdZ1 = np.multiply(dA1, A1 * (1- A1))\n",
        "\tdW1 = np.dot(dZ1, X.T) / m\n",
        "\tdb1 = np.sum(dZ1, axis = 1, keepdims = True) / m\n",
        "\t\n",
        "\tgradients = {\"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n",
        "\t\t\t\t\"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
        "\treturn gradients\n",
        "\n",
        "# Updating the weights based on the negative gradients\n",
        "def updateParameters(parameters, gradients, learningRate):\n",
        "\tparameters[\"W1\"] = parameters[\"W1\"] - learningRate * gradients[\"dW1\"]\n",
        "\tparameters[\"W2\"] = parameters[\"W2\"] - learningRate * gradients[\"dW2\"]\n",
        "\tparameters[\"b1\"] = parameters[\"b1\"] - learningRate * gradients[\"db1\"]\n",
        "\tparameters[\"b2\"] = parameters[\"b2\"] - learningRate * gradients[\"db2\"]\n",
        "\treturn parameters\n",
        "\n",
        "# Model to learn the AND truth table\n",
        "X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]]) # AND input\n",
        "Y = np.array([[0, 0, 0, 1]]) # AND output\n",
        "\n",
        "# Define model parameters\n",
        "neuronsInHiddenLayers = 2 # number of hidden layer neurons (2)\n",
        "inputFeatures = X.shape[0] # number of input features (2)\n",
        "outputFeatures = Y.shape[0] # number of output features (1)\n",
        "parameters = initializeParameters(inputFeatures, neuronsInHiddenLayers, outputFeatures)\n",
        "epoch = 100000\n",
        "learningRate = 0.01\n",
        "losses = np.zeros((epoch, 1))\n",
        "\n",
        "for i in range(epoch):\n",
        "\tlosses[i, 0], cache, A2 = forwardPropagation(X, Y, parameters)\n",
        "\tgradients = backwardPropagation(X, Y, cache)\n",
        "\tparameters = updateParameters(parameters, gradients, learningRate)\n",
        "\n",
        "# Testing\n",
        "X = np.array([[1, 1, 0, 0], [0, 1, 0, 1]]) # AND input\n",
        "cost, _, A2 = forwardPropagation(X, Y, parameters)\n",
        "prediction = (A2 > 0.5) * 1.0\n",
        "# print(A2)\n",
        "print(prediction)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 1. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}


